{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data from csv\n",
    "import csv\n",
    "input_file_path = \"/Users/k.abhishek/Documents/customer_clusters/export_all_v2.csv\"\n",
    "def load_data(input_file_path):\n",
    "    with open(input_file_path, mode ='r') as file:\n",
    "        csvFile = csv.reader(file)\n",
    "        data = list(csvFile)\n",
    "    return data\n",
    "\n",
    "data = load_data(input_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries_in_focus = [\"IN\", \"CN\", \"US\"]\n",
    "# countries_in_focus = [\"IN\"]\n",
    "country_filter = set(countries_in_focus)\n",
    "\n",
    "#filtering based on country code\n",
    "# filtered_data = [ d for d in data if d[15] in country_filter]\n",
    "\n",
    "filtered_data = data.copy()\n",
    "import random \n",
    "random.seed(42)\n",
    "random.shuffle(filtered_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3610594"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filtered_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2790777\n"
     ]
    }
   ],
   "source": [
    "#find the unique values of legal_cluster_id\n",
    "\n",
    "legal_cluster_id_set = set([d[1] for d in filtered_data])\n",
    "print(len(legal_cluster_id_set))\n",
    "\n",
    "legal_cluster_id_to_data = {}\n",
    "count_null_legal_cluster_id = 0\n",
    "for d in filtered_data:\n",
    "    if d[1] == \"null\":\n",
    "        count_null_legal_cluster_id +=1\n",
    "        continue\n",
    "    if legal_cluster_id_to_data.get(d[1]) is None:\n",
    "        legal_cluster_id_to_data[d[1]] = [d]\n",
    "    else:\n",
    "        legal_cluster_id_to_data[d[1]].append(d)    \n",
    "# filtered_data = []\n",
    "num_entries = 100000\n",
    "test_corpus = []\n",
    "comparison_corpus = []\n",
    "count_entries = 0\n",
    "for legal_cluster_id, entry in legal_cluster_id_to_data.items():\n",
    "    # if count_entries > num_entries:\n",
    "    #     break\n",
    "    if len(entry) > 1:\n",
    "        count_entries += len(entry)\n",
    "        test_corpus.append(entry[0])\n",
    "        comparison_corpus.extend(entry[1:])\n",
    "    else:\n",
    "        comparison_corpus.extend(entry)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_wise_corpus = {}\n",
    "for entry in comparison_corpus:\n",
    "    country_code = entry[15]\n",
    "    if country_wise_corpus.get(country_code) is None:\n",
    "        country_wise_corpus[country_code] = [entry]\n",
    "    else:\n",
    "        country_wise_corpus[country_code].append(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2266229894480962 1.0011390672114102\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "entries_per_legal_cluster_id = [len(v) for k,v in legal_cluster_id_to_data.items()]\n",
    "# entries_per_legal_cluster_id = [len(v) for k,v in legal_cluster_id_to_data.items() if len(v) > 1]\n",
    "mean = np.mean(entries_per_legal_cluster_id)\n",
    "std = np.std(entries_per_legal_cluster_id)\n",
    "print(mean, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3423230"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(entries_per_legal_cluster_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mean and standard deviation for the size of legal_cluster_id\n",
    "1.5947940644465912 2.2155833789806207\n",
    "\n",
    "mean and standard deviation for the size of legal_cluster_id after filtering the size of cluster to be > 1\n",
    "5.20206364596017 4.4183111370491055"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "187364"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_null_legal_cluster_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3610594\n",
      "395029\n",
      "3028201\n"
     ]
    }
   ],
   "source": [
    "print(len(filtered_data))\n",
    "print(len(test_corpus))\n",
    "print(len(comparison_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tax_numbers(arr):\n",
    "    unique_tax_numbers = set()\n",
    "    for i in range(27, 33):\n",
    "        if arr[i] and arr[i] != \"null\":\n",
    "            unique_tax_numbers.add(arr[i].lower())\n",
    "    return unique_tax_numbers\n",
    "\n",
    "def extract_bvd_numbers(arr):\n",
    "    unique_bvd_numbers = set()\n",
    "    for i in range(24, 27):\n",
    "        if arr[i] and arr[i] != \"null\":\n",
    "            unique_bvd_numbers.add(arr[i].lower())\n",
    "    return unique_bvd_numbers\n",
    "\n",
    "def match_tax(arr, unique_tax_numbers):\n",
    "    tax = extract_tax_numbers(arr)\n",
    "    for tax_num in tax:\n",
    "        if tax_num in unique_tax_numbers:\n",
    "            return unique_tax_numbers[tax_num]\n",
    "    return None\n",
    "\n",
    "def match_bvd(arr, unique_bvd_numbers):\n",
    "    bvd = extract_bvd_numbers(arr)\n",
    "    for bvd_num in bvd:\n",
    "        if bvd_num in unique_bvd_numbers:\n",
    "            return unique_bvd_numbers[bvd_num]\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_tax_numbers = {}\n",
    "unique_bvd_numbers = {}\n",
    "for idx, arr in enumerate(comparison_corpus):\n",
    "    country_code = arr[15]\n",
    "    for i in range(24, 27):\n",
    "        if arr[i] and arr[i] != \"null\":\n",
    "            if unique_bvd_numbers.get(arr[i].lower()):\n",
    "                unique_bvd_numbers[arr[i].lower()].append(arr)\n",
    "            else:\n",
    "                unique_bvd_numbers[arr[i].lower()] = [arr]\n",
    "            # unique_bvd_numbers[arr[i]] = arr\n",
    "    for i in range(27, 33):\n",
    "        if arr[i] and arr[i] != \"null\":\n",
    "            if unique_tax_numbers.get(arr[i].lower()):\n",
    "                unique_tax_numbers[arr[i].lower()].append(arr)\n",
    "            else:\n",
    "                unique_tax_numbers[arr[i].lower()] = [arr]\n",
    "            # unique_tax_numbers[arr[i]] = arr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from rapidfuzz import fuzz\n",
    "from annoy import AnnoyIndex\n",
    "\n",
    "import re\n",
    "stop_words = {'llp', 'ltd', 'llc', 'plc', 'inc', 'corp', 'nv', 'group', 'gmbh', 'company', 'corporation', 'limited'}\n",
    "delimiters = {\".\", \",\", \"-\", \"_\", \"'\", \"&\", \"/\", \"\\\\\", \":\", \";\", \"(\", \")\", \"[\", \"]\", \"{\", \"}\", \" \"}\n",
    "def split_company_name(name):\n",
    "    pattern = '|'.join(map(re.escape, delimiters))\n",
    "    words = re.split(pattern, name)\n",
    "    return [word for word in words if word]\n",
    "\n",
    "def lowercase_company_name(name):\n",
    "    return name.lower()\n",
    "\n",
    "# List of common delimiters\n",
    "def remove_stop_words(words):\n",
    "    # return \" \".join(words)\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    return \" \".join(words)\n",
    "\n",
    "def preprocess_name_2(trading_name):\n",
    "    # Lowercase the company name\n",
    "    trading_name = lowercase_company_name(trading_name)\n",
    "    # Split the company name into words\n",
    "    words = split_company_name(trading_name)\n",
    "\n",
    "    # Remove stop words\n",
    "    clean_name = remove_stop_words(words)\n",
    "    return clean_name\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_name_1(name):\n",
    "    name = name.lower()\n",
    "    name = re.sub(r'[^\\w\\s]', '', name)  # Remove punctuation\n",
    "    tokens = name.split()\n",
    "    tokens.sort()  # Sort tokens for token_sort_ratio\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "preprocess_name = preprocess_name_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def name_to_vector(name, embedding_dim=128):\n",
    "    vector = np.zeros(embedding_dim)\n",
    "    tokens = name.split()\n",
    "    for token in tokens:\n",
    "        for char in token:\n",
    "            vector[ord(char) % embedding_dim] += 1  \n",
    "    return vector\n",
    "\n",
    "def build_index(corpus, embedding_dim=128, metric='angular', num_trees=25):\n",
    "    # Preprocess the entire corpus\n",
    "    # corpus_of_names = [\"John Doe\", \"Jane Smith\", \"Alice Johnson\"]  # Add all million names here\n",
    "    corpus_of_names = [d[4] for d in corpus]\n",
    "    preprocessed_names = [preprocess_name(name) for name in corpus_of_names]\n",
    "\n",
    "    annoy_index = AnnoyIndex(embedding_dim, metric)\n",
    "    # Convert names to vectors (simple bag-of-words vectorization)\n",
    "\n",
    "\n",
    "    name_vectors = [name_to_vector(name, embedding_dim) for name in preprocessed_names]\n",
    "\n",
    "    for i, vector in enumerate(name_vectors):\n",
    "        # print(i, vector)\n",
    "        annoy_index.add_item(i, vector)\n",
    "\n",
    "    annoy_index.build(num_trees) # 50 trees\n",
    "    return annoy_index, corpus_of_names, preprocessed_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Preprocess the entire corpus\n",
    "# # corpus_of_names = [\"John Doe\", \"Jane Smith\", \"Alice Johnson\"]  # Add all million names here\n",
    "# corpus_of_names = [d[4] for d in comparison_corpus]\n",
    "# preprocessed_names = [preprocess_name(name) for name in corpus_of_names]\n",
    "\n",
    "# # Build Annoy index\n",
    "# embedding_dim = 128  # Dimensionality of the embeddings\n",
    "# annoy_index = AnnoyIndex(embedding_dim, 'angular')\n",
    "\n",
    "# # Convert names to vectors (simple bag-of-words vectorization)\n",
    "# def name_to_vector(name):\n",
    "#     vector = np.zeros(embedding_dim)\n",
    "#     tokens = name.split()\n",
    "#     for token in tokens:\n",
    "#         for char in token:\n",
    "#             vector[ord(char) % embedding_dim] += 1  \n",
    "#     return vector\n",
    "\n",
    "# name_vectors = [name_to_vector(name) for name in preprocessed_names]\n",
    "\n",
    "# for i, vector in enumerate(name_vectors):\n",
    "#     # print(i, vector)\n",
    "#     annoy_index.add_item(i, vector)\n",
    "\n",
    "# annoy_index.build(50) # 50 trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fuzzy matching score function\n",
    "function_selection = {\n",
    "    \"ratio\": 1,\n",
    "    \"partial_ratio\": 1,\n",
    "    \"token_sort_ratio\": 1,\n",
    "    \"token_set_ratio\": 1,\n",
    "    \"partial_token_sort_ratio\": 1,\n",
    "    \"partial_token_set_ratio\": 1\n",
    "}\n",
    "def combined_fuzzy_score(name1, name2):\n",
    "    ratio = fuzz.ratio(name1, name2) * function_selection[\"ratio\"]\n",
    "    partial_ratio = fuzz.partial_ratio(name1, name2) * function_selection[\"partial_ratio\"]\n",
    "    token_sort_ratio = fuzz.token_sort_ratio(name1, name2)  * function_selection[\"token_sort_ratio\"]\n",
    "    token_set_ratio = fuzz.token_set_ratio(name1, name2)    * function_selection[\"token_set_ratio\"]\n",
    "    partial_token_sort_ratio = fuzz.partial_token_sort_ratio(name1, name2) * function_selection[\"partial_token_sort_ratio\"]\n",
    "    partial_token_set_ratio = fuzz.partial_token_set_ratio(name1, name2) * function_selection[\"partial_token_set_ratio\"]\n",
    "    # print(f\"ratio: {ratio}, partial_ratio: {partial_ratio}, token_sort_ratio: {token_sort_ratio}, token_set_ratio: {token_set_ratio}, partial_token_sort_ratio: {partial_token_sort_ratio}, partial_token_set_ratio: {partial_token_set_ratio}\")\n",
    "    # print(f\"Ratios: {ratio}, {partial_ratio}, {token_sort_ratio}, {token_set_ratio}, {partial_token_sort_ratio}, {partial_token_set_ratio}\")\n",
    "    return sum([ratio, partial_ratio, token_sort_ratio, token_set_ratio, partial_token_sort_ratio, partial_token_set_ratio])/sum(function_selection.values())\n",
    "    # return max(ratio, partial_ratio, token_sort_ratio, token_set_ratio, partial_token_sort_ratio, partial_token_set_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 128\n",
    "num_trees = 25\n",
    "index_name_vector_clustered_by_country = {}\n",
    "preprocessed_names_by_country = {}\n",
    "corpus_of_names_by_country = {}\n",
    "for country_code, corpus in country_wise_corpus.items():\n",
    "    index_name_vector_clustered_by_country[country_code], corpus_of_names_by_country[country_code], preprocessed_names_by_country[country_code] = build_index(corpus, num_trees=num_trees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find best match\n",
    "def find_best_match(input_name, country_code, annoy_index, embedding_dim = 128, num_candidates=15):\n",
    "    preprocessed_input = preprocess_name(input_name)\n",
    "    input_vector = name_to_vector(preprocessed_input, embedding_dim)\n",
    "    # Retrieve nearest neighbors using Annoy\n",
    "    nearest_neighbors = annoy_index.get_nns_by_vector(input_vector, num_candidates, include_distances=False)\n",
    "    # print(nearest_neighbors)\n",
    "    best_match_score = 0\n",
    "    best_match_name = None\n",
    "    best_match_idx = None\n",
    "    preprocessed_names = preprocessed_names_by_country[country_code]\n",
    "    corpus_of_names = corpus_of_names_by_country[country_code]\n",
    "    for neighbor_idx in nearest_neighbors:\n",
    "        candidate = preprocessed_names[neighbor_idx]\n",
    "        score = combined_fuzzy_score(preprocessed_input, candidate)\n",
    "        if score > best_match_score:\n",
    "            best_match_score = score\n",
    "            best_match_name = corpus_of_names[neighbor_idx]\n",
    "            best_match_idx = neighbor_idx\n",
    "    \n",
    "    return best_match_name, best_match_score, best_match_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▋        | 64627/395029 [00:30<02:17, 2394.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['c330c792-29bf-3c51-9931-542003abcc41', 'f41ba3d6-8435-3dcb-b5e5-01454c30a1bb', 'RU00186779', 'Customer', 'LIMITED COMPANY', '9', 'STRELNIKOVA STR 9,', 'null', 'null', 'null', 'null', 'VLADIVOSTOK', 'null', 'null', 'null', 'RU', 'Russia', 'TEL', 'KZ', '7', 'null', '9147077761', '2016-06-05T17:18:57.525Z', 'null', 'null', 'null', 'null', 'null', 'null', 'null', 'null', 'null', 'null']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 352986/395029 [02:35<00:18, 2303.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['6b266ac5-f9b4-3008-833c-4cda137a33b8', 'd804d709-19e9-3231-adb4-9ef98fcc7f64', 'QA50002951', 'Customer', 'GROUP COMPANY', 'null', 'GATE NO. A2, 1ST FLOOR', '7303', 'RETAJ BUILDING, SALWA ROAD', 'null', 'null', 'DOHA', 'null', 'null', 'null', 'QA', 'Qatar', 'TEL', 'QA', '974', 'null', '44681144', '2020-06-20T08:27:05.955Z', 'null', 'null', 'null', 'null', 'null', 'null', 'null', 'null', 'null', 'null']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 395029/395029 [02:55<00:00, 2255.48it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "success = 0\n",
    "failure = 0\n",
    "not_applicable = 0\n",
    "result = {}\n",
    "false_positives = {}\n",
    "qualified_as_match_threshold = 0\n",
    "correctly_qualified_as_match_threshold = 0\n",
    "count_entries_name_matching =0\n",
    "threshold = 90\n",
    "for arr in tqdm(test_corpus):\n",
    "    legal_cluster_id = arr[1].lower()\n",
    "    matches = {}\n",
    "    #tax matching\n",
    "    tax_match = match_tax(arr, unique_tax_numbers)\n",
    "    if tax_match:\n",
    "        success += 1\n",
    "        matches[1] = tax_match\n",
    "        result[str(arr)] = matches\n",
    "        continue\n",
    "    #bvd matching\n",
    "    bvd_match = match_bvd(arr, unique_bvd_numbers)\n",
    "    if bvd_match:\n",
    "        success += 1\n",
    "        matches[1] = bvd_match\n",
    "        result[str(arr)] = matches\n",
    "        continue\n",
    "    # print(\"Name matching\")\n",
    "    company_name = arr[4]\n",
    "    count_entries_name_matching += 1\n",
    "    country_code = arr[15]\n",
    "    annoy_index = index_name_vector_clustered_by_country[country_code]\n",
    "    corpus = country_wise_corpus[country_code]\n",
    "    best_match_name, best_match_score, best_match_idx = find_best_match(company_name, country_code, annoy_index, embedding_dim=128, num_candidates=15)\n",
    "    best_matched_entry = None\n",
    "    if best_match_idx == None:\n",
    "        not_applicable +=1\n",
    "        print(arr)\n",
    "    if best_match_idx and best_match_score > threshold:\n",
    "        # best_matched_entry = comparison_corpus[best_match_idx]\n",
    "        best_matched_entry = corpus[best_match_idx]\n",
    "        # print(best_match_score)\n",
    "        qualified_as_match_threshold +=1\n",
    "        if legal_cluster_id == best_matched_entry[1]:\n",
    "            success +=1\n",
    "            correctly_qualified_as_match_threshold +=1\n",
    "        else:\n",
    "            false_positives[str(arr)] = best_matched_entry\n",
    "            failure +=1\n",
    "    else:\n",
    "        failure +=1\n",
    "    result[str(arr)] = best_matched_entry\n",
    "    # print(idx+1, success, failure, success + failure)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "322201 72828\n",
      "206515 185516\n"
     ]
    }
   ],
   "source": [
    "print(success, failure)\n",
    "print(qualified_as_match_threshold, correctly_qualified_as_match_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "311033 83997\n",
    "205553 177854\n",
    "0.7873655165430473\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8156388518311314"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "success/(success + failure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "258344"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_entries_name_matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8983173135123357\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20999"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision = correctly_qualified_as_match_threshold/qualified_as_match_threshold\n",
    "print(precision)\n",
    "qualified_as_match_threshold - correctly_qualified_as_match_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "save_result_filename = \"complete_data_\" + \"false_positives_\" + str(embedding_dim) + \"_\" + str(function_selection['ratio']) + \"_\" + str(function_selection['partial_ratio']) + \"_\" + str(function_selection['token_sort_ratio']) + \"_\" + str(function_selection['token_set_ratio']) + \"_\" + str(function_selection['partial_token_sort_ratio']) + \"_\" + str(function_selection['partial_token_set_ratio']) + \"_threshold_\" + str(threshold) + \".json\"\n",
    "# save_result_filename = \"result_name_matching_threshold_85.json\"\n",
    "save_folder_path = \"/Users/k.abhishek/Documents/customer_clusters/experiments/approximate_match_approach/results\"\n",
    "save_result_file_path = os.path.join(save_folder_path, save_result_filename)\n",
    "\n",
    "def save_result(result, save_result_file_path):\n",
    "    import json\n",
    "    with open(save_result_file_path, 'w') as f:\n",
    "        json.dump(result, f)\n",
    "\n",
    "save_result(false_positives, save_result_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
