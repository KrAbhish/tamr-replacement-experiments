{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Preprocessing and Tokenization\n",
    "Before performing fuzzy matching, preprocess and tokenize the names to standardize the format. This can include:\n",
    "\n",
    "Lowercasing all names.\n",
    "Removing punctuation and special characters.\n",
    "Splitting names into tokens.\n",
    "2. Indexing for Efficient Lookups\n",
    "Use an indexing method to narrow down the number of comparisons:\n",
    "\n",
    "Trie (Prefix Tree): Efficiently store and search large sets of strings. This can quickly eliminate many non-matching names.\n",
    "Inverted Index: Create an inverted index where each token points to the list of names containing that token. This can significantly reduce the search space.\n",
    "3. Approximate Nearest Neighbors (ANN)\n",
    "Use an ANN algorithm for efficient similarity search:\n",
    "\n",
    "FAISS (Facebook AI Similarity Search): An efficient library for ANN search. It can handle millions of names and provide fast approximate nearest neighbor lookups.\n",
    "Annoy (Approximate Nearest Neighbors Oh Yeah): Another efficient library for ANN search, suitable for large datasets.\n",
    "4. Combining Fuzzy Matching Methods\n",
    "Use a combination of fuzzy matching methods to compute similarity scores and identify the best matches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess and tokenize names to standardize format.\n",
    "Index the names using a trie or inverted index for efficient lookups.\n",
    "Use ANN algorithms (FAISS or Annoy) to narrow down the search space.\n",
    "Apply combined fuzzy matching methods to determine the best match from the reduced candidate list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from rapidfuzz import fuzz\n",
    "from annoy import AnnoyIndex\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_name(name):\n",
    "    name = name.lower()\n",
    "    name = re.sub(r'[^\\w\\s]', '', name)  # Remove punctuation\n",
    "    tokens = name.split()\n",
    "    tokens.sort()  # Sort tokens for token_sort_ratio\n",
    "    return ' '.join(tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 0. 0. 0. 1. 2. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "1 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 1. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "2 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 1. 0. 1. 0. 1. 0. 0. 1. 1. 1. 0. 1. 0. 2. 2. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocess the entire corpus\n",
    "corpus_of_names = [\"John Doe\", \"Jane Smith\", \"Alice Johnson\"]  # Add all million names here\n",
    "preprocessed_names = [preprocess_name(name) for name in corpus_of_names]\n",
    "\n",
    "# Build Annoy index\n",
    "embedding_dim = 128  # Dimensionality of the embeddings\n",
    "annoy_index = AnnoyIndex(embedding_dim, 'angular')\n",
    "\n",
    "# Convert names to vectors (simple bag-of-words vectorization)\n",
    "def name_to_vector(name):\n",
    "    vector = np.zeros(embedding_dim)\n",
    "    tokens = name.split()\n",
    "    for token in tokens:\n",
    "        for char in token:\n",
    "            vector[ord(char) % embedding_dim] += 1  # Simple hashing of characters\n",
    "    return vector\n",
    "\n",
    "name_vectors = [name_to_vector(name) for name in preprocessed_names]\n",
    "\n",
    "for i, vector in enumerate(name_vectors):\n",
    "    print(i, vector)\n",
    "    annoy_index.add_item(i, vector)\n",
    "\n",
    "# Build the index (number of trees can be adjusted for accuracy/speed trade-off)\n",
    "annoy_index.build(10)  # 10 trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fuzzy matching score function\n",
    "function_selection = {\n",
    "    \"ratio\": 1,\n",
    "    \"partial_ratio\": 0,\n",
    "    \"token_sort_ratio\": 1,\n",
    "    \"token_set_ratio\": 1,\n",
    "    \"partial_token_sort_ratio\": 0,\n",
    "    \"partial_token_set_ratio\": 0\n",
    "}\n",
    "def combined_fuzzy_score(name1, name2):\n",
    "    ratio = fuzz.ratio(name1, name2) * function_selection[\"ratio\"]\n",
    "    partial_ratio = fuzz.partial_ratio(name1, name2) * function_selection[\"partial_ratio\"]\n",
    "    token_sort_ratio = fuzz.token_sort_ratio(name1, name2)  * function_selection[\"token_sort_ratio\"]\n",
    "    token_set_ratio = fuzz.token_set_ratio(name1, name2)    * function_selection[\"token_set_ratio\"]\n",
    "    partial_token_sort_ratio = fuzz.partial_token_sort_ratio(name1, name2) * function_selection[\"partial_token_sort_ratio\"]\n",
    "    partial_token_set_ratio = fuzz.partial_token_set_ratio(name1, name2) * function_selection[\"partial_token_set_ratio\"]\n",
    "    print(f\"ratio: {ratio}, partial_ratio: {partial_ratio}, token_sort_ratio: {token_sort_ratio}, token_set_ratio: {token_set_ratio}, partial_token_sort_ratio: {partial_token_sort_ratio}, partial_token_set_ratio: {partial_token_set_ratio}\")\n",
    "    # print(f\"Ratios: {ratio}, {partial_ratio}, {token_sort_ratio}, {token_set_ratio}, {partial_token_sort_ratio}, {partial_token_set_ratio}\")\n",
    "    return sum([ratio, partial_ratio, token_sort_ratio, token_set_ratio, partial_token_sort_ratio, partial_token_set_ratio])/sum(function_selection.values())\n",
    "    return max(ratio, partial_ratio, token_sort_ratio, token_set_ratio, partial_token_sort_ratio, partial_token_set_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find best match\n",
    "def find_best_match(input_name, num_candidates=10):\n",
    "    preprocessed_input = preprocess_name(input_name)\n",
    "    input_vector = name_to_vector(preprocessed_input)\n",
    "    \n",
    "    # Retrieve nearest neighbors using Annoy\n",
    "    nearest_neighbors = annoy_index.get_nns_by_vector(input_vector, num_candidates, include_distances=False)\n",
    "    print(nearest_neighbors)\n",
    "    best_match_score = 0\n",
    "    best_match_name = None\n",
    "    \n",
    "    for neighbor_idx in nearest_neighbors:\n",
    "        candidate = preprocessed_names[neighbor_idx]\n",
    "        score = combined_fuzzy_score(preprocessed_input, candidate)\n",
    "        if score > best_match_score:\n",
    "            best_match_score = score\n",
    "            best_match_name = corpus_of_names[neighbor_idx]\n",
    "    \n",
    "    return best_match_name, best_match_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2, 1]\n",
      "ratio: 88.88888888888889, partial_ratio: 0.0, token_sort_ratio: 88.88888888888889, token_set_ratio: 100.0, partial_token_sort_ratio: 0.0, partial_token_set_ratio: 0.0\n",
      "ratio: 60.86956521739131, partial_ratio: 0.0, token_sort_ratio: 60.86956521739131, token_set_ratio: 60.869565217391305, partial_token_sort_ratio: 0.0, partial_token_set_ratio: 0.0\n",
      "ratio: 40.0, partial_ratio: 0.0, token_sort_ratio: 40.0, token_set_ratio: 40.0, partial_token_sort_ratio: 0.0, partial_token_set_ratio: 0.0\n",
      "Best match: John Doe with score 92.5925925925926\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "input_name = \"John A. Doe\"\n",
    "best_match, score = find_best_match(input_name)\n",
    "print(f\"Best match: {best_match} with score {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing:\n",
    "\n",
    "The preprocess_name function normalizes the names by lowercasing and removing punctuation.\n",
    "Names are then tokenized and sorted to handle different orderings.\n",
    "Vectorization:\n",
    "\n",
    "name_to_vector converts names into simple bag-of-words vectors based on character hashing.\n",
    "Indexing with Annoy:\n",
    "\n",
    "An Annoy index is built using the vectors of preprocessed names.\n",
    "annoy_index.build builds the index with a specified number of trees. More trees increase accuracy at the cost of speed.\n",
    "Fuzzy Matching:\n",
    "\n",
    "combined_fuzzy_score combines several fuzzy matching methods from rapidfuzz to compute a comprehensive similarity score.\n",
    "Finding the Best Match:\n",
    "\n",
    "find_best_match preprocesses the input name, converts it to a vector, retrieves the nearest neighbors from the Annoy index, and then applies the fuzzy matching methods to find the best match."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
